{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Ohvd-ZSv7OwU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "data_path = \"/content/drive/MyDrive/zindidata/french1.txt\"\n",
        "data_path2 = \"/content/drive/MyDrive/zindidata/fon1.txt\"\n",
        "# Defining lines as a list of each line\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().strip().split('\\n')\n",
        "with open(data_path2, 'r', encoding='utf-8') as f:\n",
        "  lines2 = f.read().strip().split('\\n')\n",
        "\n",
        "lines = [\" \".join(re.findall(r\"[A-Za-z0-9]+\",line)) for line in lines]\n",
        "lines2 = [\" \".join(re.findall(r\"[A-Za-z0-9]+\",line)) for line in lines2]#[re.sub(r\"%s||<|>|%|[a-z]|[A-Z]|_\",'',line) for line in lines2]\n",
        "#print(lines[:5])\n",
        "\n",
        "# Grouping lines by response pair\n",
        "pairs = list(zip(lines,lines2))\n",
        "random.shuffle(pairs)\n",
        "print(len(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZUjcB-c6HWa",
        "outputId": "365a87ff-ffc2-4334-b7a2-71468cdda91f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diviser les paires en données d'entrée et de sortie\n",
        "input_sequences = [pair[0] for pair in pairs]\n",
        "target_sequences = [pair[1] for pair in pairs]\n"
      ],
      "metadata": {
        "id": "d-uma3Dq7VLF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assurez-vous que toutes les séquences ont la même longueur\n",
        "max_len_input = max(len(seq) for seq in input_sequences)\n",
        "max_len_target = max(len(seq) for seq in target_sequences)"
      ],
      "metadata": {
        "id": "gD7rubWE_dSF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization : convertir les mots en indices\n",
        "\n",
        "tokenizer_inputs = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer_outputs = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer_inputs.fit_on_texts(input_sequences)\n",
        "tokenizer_outputs.fit_on_texts(target_sequences)\n",
        "\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_sequences)\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_sequences)\n"
      ],
      "metadata": {
        "id": "YfuJd-0W7YJw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remarque : assurez-vous d'ajuster vos séquences d'entrée et de sortie à la même longueur maximale\n",
        "encoder_inputs_padded = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_len_input, padding='post')\n",
        "decoder_inputs_padded = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n"
      ],
      "metadata": {
        "id": "yoeCb6EZ7ewd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créez les séquences de sortie décalées d'une position vers la droite\n",
        "# Cela est nécessaire car le modèle prédit le mot suivant à partir du mot actuel\n",
        "decoder_outputs_padded = np.zeros_like(decoder_inputs_padded)\n",
        "decoder_outputs_padded[:, 0:-1] = decoder_inputs_padded[:, 1:]\n",
        "decoder_outputs_padded[:, -1] = 0  # Remplacer le dernier token par un token de padding\n"
      ],
      "metadata": {
        "id": "EMP4X83aASnt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Création du modèle\n",
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "metadata": {
        "id": "kxicPB8d7mry"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodeur\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_len_input,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(len(tokenizer_inputs.word_index) + 1, embedding_dim)\n",
        "encoder_lstm = tf.keras.layers.LSTM(units, return_state=True)\n",
        "\n",
        "encoder_embedding_output = encoder_embedding(encoder_inputs)\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "XlLETXhF7raY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Décodeur\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(max_len_target,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(len(tokenizer_outputs.word_index) + 1, embedding_dim)\n",
        "decoder_lstm = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_dense = tf.keras.layers.Dense(len(tokenizer_outputs.word_index) + 1, activation='softmax')\n",
        "\n",
        "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense(decoder_lstm_output)"
      ],
      "metadata": {
        "id": "bPSMNZ2k7wUk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle complet\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "fsdVGYDz71Z9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Entrainement du modèle\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#model.fit([encoder_inputs_padded, decoder_inputs_padded], np.expand_dims(decoder_outputs_padded, -1), batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egOwLcsM76Vb",
        "outputId": "8746de29-c5dc-4cc8-d15a-1df75ea08f6d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "665/665 [==============================] - 371s 550ms/step - loss: 0.1878 - accuracy: 0.9738 - val_loss: 0.1204 - val_accuracy: 0.9782\n",
            "Epoch 2/10\n",
            "665/665 [==============================] - 368s 553ms/step - loss: 0.1168 - accuracy: 0.9783 - val_loss: 0.1136 - val_accuracy: 0.9787\n",
            "Epoch 3/10\n",
            "665/665 [==============================] - 367s 552ms/step - loss: 0.1107 - accuracy: 0.9787 - val_loss: 0.1089 - val_accuracy: 0.9790\n",
            "Epoch 4/10\n",
            "665/665 [==============================] - 368s 553ms/step - loss: 0.1060 - accuracy: 0.9791 - val_loss: 0.1050 - val_accuracy: 0.9793\n",
            "Epoch 5/10\n",
            "665/665 [==============================] - 368s 553ms/step - loss: 0.1017 - accuracy: 0.9794 - val_loss: 0.1022 - val_accuracy: 0.9796\n",
            "Epoch 6/10\n",
            "665/665 [==============================] - 367s 552ms/step - loss: 0.0976 - accuracy: 0.9798 - val_loss: 0.0992 - val_accuracy: 0.9799\n",
            "Epoch 7/10\n",
            "665/665 [==============================] - 369s 556ms/step - loss: 0.0937 - accuracy: 0.9802 - val_loss: 0.0953 - val_accuracy: 0.9803\n",
            "Epoch 8/10\n",
            "665/665 [==============================] - 369s 554ms/step - loss: 0.0890 - accuracy: 0.9807 - val_loss: 0.0931 - val_accuracy: 0.9806\n",
            "Epoch 9/10\n",
            "665/665 [==============================] - 359s 540ms/step - loss: 0.0842 - accuracy: 0.9814 - val_loss: 0.0878 - val_accuracy: 0.9815\n",
            "Epoch 10/10\n",
            "665/665 [==============================] - 368s 554ms/step - loss: 0.0763 - accuracy: 0.9828 - val_loss: 0.0820 - val_accuracy: 0.9826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e23d0577160>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Stda3W6y5xuz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sauvegarde du modèle\n",
        "model.save('translation_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Entrainement du modèle avec enregistrement de l'historique\n",
        "#history = model.fit([encoder_inputs_padded, decoder_inputs_padded], np.expand_dims(decoder_outputs_padded, -1), batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Affichage de la courbe d'apprentissage\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "hDpdSgayQSFS",
        "outputId": "9a1d660c-d860-45d7-edcb-14c7da2f7aae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a03c54c09ce3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Affichage de la courbe d'apprentissage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle entraîné\n",
        "model = tf.keras.models.load_model('translation_model.h5')\n",
        "\n",
        "# Définir encoder_model et decoder_model\n",
        "# Placer ici le code pour définir encoder_model et decoder_model\n",
        "\n",
        "# Fonction pour effectuer une traduction\n",
        "def translate_sentence(input_sentence):\n",
        "    # Prétraiter l'entrée\n",
        "    input_sequence = tokenizer_inputs.texts_to_sequences([input_sentence])\n",
        "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=max_len_input, padding='post')\n",
        "\n",
        "    # Encodeur\n",
        "    encoder_input = input_sequence\n",
        "    encoder_output, encoder_state_h, encoder_state_c = encoder_model.predict(encoder_input)\n",
        "    encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "    # Début de la séquence de sortie\n",
        "    target_sequence = np.zeros((1, 1))\n",
        "    target_sequence[0, 0] = tokenizer_outputs.word_index['<start>']\n",
        "\n",
        "    # Traduction de la phrase\n",
        "    translated_sentence = ''\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        decoder_output, decoder_state_h, decoder_state_c = decoder_model.predict([target_sequence] + encoder_states)\n",
        "\n",
        "        # Index du mot prédit\n",
        "        sampled_token_index = np.argmax(decoder_output[0, -1, :])\n",
        "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        # Arrêt si on atteint la fin de la phrase ou si la longueur maximale est atteinte\n",
        "        if sampled_word == '<end>' or len(translated_sentence.split()) > max_len_target:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            translated_sentence += sampled_word + ' '\n",
        "\n",
        "        # Mise à jour de l'état du décodeur\n",
        "        encoder_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "        # Mettre à jour la séquence de sortie\n",
        "        target_sequence = np.zeros((1, 1))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "# Exemple d'utilisation de la fonction de traduction\n",
        "input_sentence = 'bonjour'\n",
        "translated_sentence = translate_sentence(input_sentence)\n",
        "print('Input Sentence:', input_sentence)\n",
        "print('Translated Sentence:', translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ND45LnsXQ_Ue",
        "outputId": "002d5b68-eeee-496f-9ecd-3b3ab7035677"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at translation_model.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-faf38e0eac46>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Charger le modèle entraîné\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'translation_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Définir encoder_model et decoder_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Placer ici le code pour définir encoder_model et decoder_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at translation_model.h5"
          ]
        }
      ]
    }
  ]
}